{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Finance - Fundamentals\n",
    "\n",
    "Welcome to this comprehensive tutorial on machine learning fundamentals with applications in finance. By the end of this notebook, you will understand:\n",
    "\n",
    "- Core machine learning concepts and terminology\n",
    "- How to work with financial data in Python\n",
    "- Key ML algorithms: regression, classification, and clustering\n",
    "- Best practices for evaluating models on financial data\n",
    "- How to build a simple ML-based trading strategy\n",
    "\n",
    "**Prerequisites**: Intermediate Python knowledge (functions, classes, basic syntax)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Python Essentials\n",
    "\n",
    "First, let's install and import all the libraries we'll need throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install numpy pandas matplotlib seaborn scikit-learn yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Financial data\n",
    "import yfinance as yf\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick NumPy and Pandas Refresher\n",
    "\n",
    "Machine learning relies heavily on numerical operations. Let's quickly review the key data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy: Efficient numerical arrays\n",
    "prices = np.array([100, 102, 101, 105, 103, 108])\n",
    "print(\"Stock prices:\", prices)\n",
    "print(\"Mean price:\", np.mean(prices))\n",
    "print(\"Standard deviation:\", np.std(prices))\n",
    "\n",
    "# Calculate daily returns: (today - yesterday) / yesterday\n",
    "returns = np.diff(prices) / prices[:-1]\n",
    "print(\"Daily returns:\", returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas: DataFrames for tabular data\n",
    "df_example = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=6),\n",
    "    'Price': prices,\n",
    "    'Volume': [1000, 1200, 900, 1500, 1100, 1300]\n",
    "})\n",
    "df_example['Returns'] = df_example['Price'].pct_change()\n",
    "print(df_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Machine Learning\n",
    "\n",
    "### What is Machine Learning?\n",
    "\n",
    "Machine Learning (ML) is a subset of artificial intelligence where computers learn patterns from data without being explicitly programmed. Instead of writing rules, we provide examples and let the algorithm discover the rules.\n",
    "\n",
    "### Types of Machine Learning\n",
    "\n",
    "| Type | Description | Financial Example |\n",
    "|------|-------------|-------------------|\n",
    "| **Supervised Learning** | Learn from labeled data (input → output) | Predict stock price direction |\n",
    "| **Unsupervised Learning** | Find patterns in unlabeled data | Group similar stocks together |\n",
    "| **Reinforcement Learning** | Learn through trial and error | Algorithmic trading agents |\n",
    "\n",
    "### The ML Workflow\n",
    "\n",
    "```\n",
    "Data Collection → Data Preprocessing → Feature Engineering → \n",
    "Model Training → Model Evaluation → Deployment\n",
    "```\n",
    "\n",
    "### Key Terminology\n",
    "\n",
    "- **Features (X)**: Input variables used to make predictions (e.g., past prices, volume, indicators)\n",
    "- **Labels/Target (y)**: The output we want to predict (e.g., tomorrow's price, buy/sell signal)\n",
    "- **Training Data**: Data used to teach the model\n",
    "- **Test Data**: Data used to evaluate model performance (never seen during training)\n",
    "- **Overfitting**: Model memorizes training data but fails on new data\n",
    "- **Underfitting**: Model is too simple to capture patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: Overfitting vs Good Fit\n",
    "np.random.seed(42)\n",
    "X_demo = np.linspace(0, 10, 20)\n",
    "y_demo = 2 * X_demo + 1 + np.random.randn(20) * 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Underfitting (too simple)\n",
    "axes[0].scatter(X_demo, y_demo, alpha=0.7)\n",
    "axes[0].axhline(y=np.mean(y_demo), color='red', linewidth=2)\n",
    "axes[0].set_title('Underfitting\\n(Model too simple)', fontsize=12)\n",
    "\n",
    "# Good fit\n",
    "axes[1].scatter(X_demo, y_demo, alpha=0.7)\n",
    "z = np.polyfit(X_demo, y_demo, 1)\n",
    "axes[1].plot(X_demo, np.poly1d(z)(X_demo), color='green', linewidth=2)\n",
    "axes[1].set_title('Good Fit\\n(Captures the pattern)', fontsize=12)\n",
    "\n",
    "# Overfitting (too complex)\n",
    "axes[2].scatter(X_demo, y_demo, alpha=0.7)\n",
    "z = np.polyfit(X_demo, y_demo, 15)\n",
    "X_smooth = np.linspace(0, 10, 100)\n",
    "axes[2].plot(X_smooth, np.poly1d(z)(X_smooth), color='red', linewidth=2)\n",
    "axes[2].set_title('Overfitting\\n(Memorizes noise)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Working with Financial Data\n",
    "\n",
    "Let's fetch real stock market data and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch historical data for Apple, Google, and Bitcoin\n",
    "tickers = ['AAPL', 'GOOGL', 'BTC-USD']\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "# Download data\n",
    "data = {}\n",
    "for ticker in tickers:\n",
    "    data[ticker] = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    print(f\"{ticker}: {len(data[ticker])} trading days\")\n",
    "\n",
    "# We'll primarily use Apple for our examples\n",
    "df = data['AAPL'].copy()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the data\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"\\nColumn types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before building models, we need to understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Price over time\n",
    "axes[0, 0].plot(df.index, df['Close'], color='blue', linewidth=1)\n",
    "axes[0, 0].set_title('AAPL Closing Price Over Time', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "\n",
    "# Volume over time\n",
    "axes[0, 1].bar(df.index, df['Volume'], color='gray', alpha=0.7, width=1)\n",
    "axes[0, 1].set_title('Trading Volume Over Time', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Volume')\n",
    "\n",
    "# Daily returns distribution\n",
    "daily_returns = df['Close'].pct_change().dropna()\n",
    "axes[1, 0].hist(daily_returns, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "axes[1, 0].set_title('Distribution of Daily Returns', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Daily Return')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Candlestick-style: High-Low range\n",
    "df['Range'] = df['High'] - df['Low']\n",
    "axes[1, 1].plot(df.index, df['Range'], color='orange', linewidth=0.5)\n",
    "axes[1, 1].set_title('Daily Price Range (Volatility Indicator)', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('High - Low ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering for Finance\n",
    "\n",
    "Raw prices aren't very useful for ML. We need to create meaningful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create technical indicators and features for ML models.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Returns\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # Moving Averages\n",
    "    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "    \n",
    "    # Moving Average Crossover Signal\n",
    "    df['SMA_Cross'] = (df['SMA_5'] > df['SMA_20']).astype(int)\n",
    "    \n",
    "    # Volatility (20-day rolling standard deviation of returns)\n",
    "    df['Volatility'] = df['Returns'].rolling(window=20).std()\n",
    "    \n",
    "    # Relative Strength Index (RSI)\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Price relative to moving average\n",
    "    df['Price_SMA20_Ratio'] = df['Close'] / df['SMA_20']\n",
    "    \n",
    "    # Momentum (5-day price change)\n",
    "    df['Momentum_5'] = df['Close'].pct_change(periods=5)\n",
    "    \n",
    "    # Volume features\n",
    "    df['Volume_SMA_20'] = df['Volume'].rolling(window=20).mean()\n",
    "    df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA_20']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df = create_features(df)\n",
    "print(\"New features created:\")\n",
    "print(df.columns.tolist())\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some technical indicators\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Price with Moving Averages\n",
    "recent = df.iloc[-252:]  # Last year of data\n",
    "axes[0].plot(recent.index, recent['Close'], label='Close', linewidth=1)\n",
    "axes[0].plot(recent.index, recent['SMA_5'], label='SMA 5', linewidth=1, alpha=0.8)\n",
    "axes[0].plot(recent.index, recent['SMA_20'], label='SMA 20', linewidth=1, alpha=0.8)\n",
    "axes[0].plot(recent.index, recent['SMA_50'], label='SMA 50', linewidth=1, alpha=0.8)\n",
    "axes[0].set_title('Price with Moving Averages', fontsize=12)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "\n",
    "# RSI\n",
    "axes[1].plot(recent.index, recent['RSI'], color='purple', linewidth=1)\n",
    "axes[1].axhline(y=70, color='red', linestyle='--', alpha=0.7, label='Overbought (70)')\n",
    "axes[1].axhline(y=30, color='green', linestyle='--', alpha=0.7, label='Oversold (30)')\n",
    "axes[1].set_title('Relative Strength Index (RSI)', fontsize=12)\n",
    "axes[1].set_ylabel('RSI')\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "# Volatility\n",
    "axes[2].plot(recent.index, recent['Volatility'] * 100, color='orange', linewidth=1)\n",
    "axes[2].set_title('20-Day Rolling Volatility', fontsize=12)\n",
    "axes[2].set_ylabel('Volatility (%)')\n",
    "axes[2].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Data Preprocessing\n",
    "\n",
    "Before feeding data into ML models, we need to clean and prepare it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values by dropping rows with NaN\n",
    "# (These occur at the start due to rolling calculations)\n",
    "df_clean = df.dropna().copy()\n",
    "print(f\"Rows after cleaning: {len(df_clean)}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Labels for Classification\n",
    "\n",
    "For classification, we need to create target labels. A common task is predicting whether the price will go up or down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable: Will price go UP (1) or DOWN (0) tomorrow?\n",
    "df_clean['Target'] = (df_clean['Close'].shift(-1) > df_clean['Close']).astype(int)\n",
    "\n",
    "# Remove the last row (no future data to create target)\n",
    "df_clean = df_clean.iloc[:-1]\n",
    "\n",
    "print(\"Target distribution:\")\n",
    "print(df_clean['Target'].value_counts())\n",
    "print(f\"\\nUp days: {df_clean['Target'].sum() / len(df_clean) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Many ML algorithms work better when features are on similar scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for our models\n",
    "feature_columns = [\n",
    "    'Returns', 'Volatility', 'RSI', 'Price_SMA20_Ratio',\n",
    "    'Momentum_5', 'Volume_Ratio', 'SMA_Cross'\n",
    "]\n",
    "\n",
    "X = df_clean[feature_columns].values\n",
    "y = df_clean['Target'].values\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target vector shape:\", y.shape)\n",
    "\n",
    "# Show feature statistics before scaling\n",
    "print(\"\\nFeature statistics (before scaling):\")\n",
    "print(df_clean[feature_columns].describe().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler: transforms data to have mean=0 and std=1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Example: Scale the features\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"After StandardScaler:\")\n",
    "print(f\"Mean of each feature: {X_scaled.mean(axis=0).round(4)}\")\n",
    "print(f\"Std of each feature: {X_scaled.std(axis=0).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split for Time Series\n",
    "\n",
    "**IMPORTANT**: For time series data, we cannot randomly split! We must preserve temporal order to avoid look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG way (don't do this for time series!)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CORRECT way: Use temporal split\n",
    "split_index = int(len(X) * 0.8)  # 80% train, 20% test\n",
    "\n",
    "X_train = X[:split_index]\n",
    "X_test = X[split_index:]\n",
    "y_train = y[:split_index]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "# Scale AFTER splitting (fit on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use same parameters!\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTrain date range: {df_clean.index[0].date()} to {df_clean.index[split_index-1].date()}\")\n",
    "print(f\"Test date range: {df_clean.index[split_index].date()} to {df_clean.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Supervised Learning - Regression\n",
    "\n",
    "Regression predicts a continuous value. Let's predict next-day returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regression, our target is the next day's return (continuous)\n",
    "df_reg = df_clean.copy()\n",
    "df_reg['Target_Return'] = df_reg['Returns'].shift(-1)\n",
    "df_reg = df_reg.dropna()\n",
    "\n",
    "X_reg = df_reg[feature_columns].values\n",
    "y_reg = df_reg['Target_Return'].values\n",
    "\n",
    "# Time series split\n",
    "split_idx = int(len(X_reg) * 0.8)\n",
    "X_train_reg = X_reg[:split_idx]\n",
    "X_test_reg = X_reg[split_idx:]\n",
    "y_train_reg = y_reg[:split_idx]\n",
    "y_test_reg = y_reg[split_idx:]\n",
    "\n",
    "# Scale features\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = lr_model.predict(X_train_reg_scaled)\n",
    "y_pred_test = lr_model.predict(X_test_reg_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Linear Regression Results:\")\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  R² Score: {r2_score(y_train_reg, y_pred_train):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train_reg, y_pred_train)):.6f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  R² Score: {r2_score(y_test_reg, y_pred_test):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_test)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the model: Feature coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Feature Importance (by coefficient magnitude):\")\n",
    "print(coef_df)\n",
    "\n",
    "# Visualize coefficients\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['green' if c > 0 else 'red' for c in coef_df['Coefficient']]\n",
    "plt.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Linear Regression Coefficients')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test_reg, y_pred_test, alpha=0.5, s=10)\n",
    "axes[0].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "             [y_test_reg.min(), y_test_reg.max()], \n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Returns')\n",
    "axes[0].set_ylabel('Predicted Returns')\n",
    "axes[0].set_title('Predicted vs Actual Returns')\n",
    "axes[0].legend()\n",
    "\n",
    "# Time series comparison\n",
    "test_dates = df_reg.index[split_idx:]\n",
    "axes[1].plot(test_dates[:100], y_test_reg[:100], label='Actual', alpha=0.8)\n",
    "axes[1].plot(test_dates[:100], y_pred_test[:100], label='Predicted', alpha=0.8)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Returns')\n",
    "axes[1].set_title('Actual vs Predicted Returns (First 100 Test Days)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "Notice the R² is likely low or even negative. This is expected! Financial markets are notoriously hard to predict, especially with simple models. This is why:\n",
    "\n",
    "1. Markets are efficient - prices quickly incorporate available information\n",
    "2. There's significant randomness (noise) in short-term movements\n",
    "3. Simple linear relationships often don't capture market dynamics\n",
    "\n",
    "But even slight predictive power can be valuable in trading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Supervised Learning - Classification\n",
    "\n",
    "Classification predicts discrete categories. Let's predict whether price goes UP or DOWN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already prepared classification data earlier\n",
    "print(\"Classification task: Predict if tomorrow's price is UP (1) or DOWN (0)\")\n",
    "print(f\"\\nTraining samples: {len(X_train_scaled)}\")\n",
    "print(f\"Test samples: {len(X_test_scaled)}\")\n",
    "print(f\"\\nClass balance in training: {np.mean(y_train):.2%} UP days\")\n",
    "print(f\"Class balance in test: {np.mean(y_test):.2%} UP days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Despite its name, Logistic Regression is a classification algorithm. It predicts probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "y_prob_log = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability of UP\n",
    "\n",
    "# Evaluate\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_log):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_log):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_log, target_names=['DOWN', 'UP']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Decision Trees create interpretable rules - perfect for understanding what drives predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree (with limited depth to prevent overfitting)\n",
    "dt_model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = dt_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Decision Tree Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_dt):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance for Decision Tree\n",
    "importance_dt = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(importance_dt['Feature'], importance_dt['Importance'], color='steelblue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Decision Tree Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest is an ensemble of many decision trees - usually more robust than a single tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all classification models\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_log),\n",
    "        accuracy_score(y_test, y_pred_dt),\n",
    "        accuracy_score(y_test, y_pred_rf)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test, y_pred_log),\n",
    "        precision_score(y_test, y_pred_dt),\n",
    "        precision_score(y_test, y_pred_rf)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, y_pred_log),\n",
    "        recall_score(y_test, y_pred_dt),\n",
    "        recall_score(y_test, y_pred_rf)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(models_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix for best model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, y_pred) in zip(axes, \n",
    "    [('Logistic Regression', y_pred_log), \n",
    "     ('Decision Tree', y_pred_dt), \n",
    "     ('Random Forest', y_pred_rf)]):\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(f'{name}\\nAccuracy: {accuracy_score(y_test, y_pred):.2%}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Unsupervised Learning\n",
    "\n",
    "Unsupervised learning finds patterns without labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "Let's group stocks by their behavior patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for multiple stocks\n",
    "stock_features = {}\n",
    "tickers_for_clustering = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'META', \n",
    "                           'NVDA', 'JPM', 'BAC', 'XOM', 'CVX',\n",
    "                           'JNJ', 'PFE', 'KO', 'PEP', 'WMT']\n",
    "\n",
    "print(\"Downloading stock data for clustering...\")\n",
    "for ticker in tickers_for_clustering:\n",
    "    try:\n",
    "        stock_data = yf.download(ticker, start='2023-01-01', end='2024-12-31', progress=False)\n",
    "        if len(stock_data) > 100:\n",
    "            returns = stock_data['Close'].pct_change().dropna()\n",
    "            stock_features[ticker] = {\n",
    "                'mean_return': returns.mean(),\n",
    "                'volatility': returns.std(),\n",
    "                'sharpe': returns.mean() / returns.std() if returns.std() > 0 else 0,\n",
    "                'skewness': returns.skew(),\n",
    "                'max_drawdown': (stock_data['Close'] / stock_data['Close'].cummax() - 1).min()\n",
    "            }\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Create DataFrame\n",
    "cluster_df = pd.DataFrame(stock_features).T\n",
    "print(f\"\\nCollected data for {len(cluster_df)} stocks\")\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for clustering\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster = scaler_cluster.fit_transform(cluster_df)\n",
    "\n",
    "# Apply K-Means with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_df['Cluster'] = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "print(\"Cluster assignments:\")\n",
    "for cluster in sorted(cluster_df['Cluster'].unique()):\n",
    "    stocks = cluster_df[cluster_df['Cluster'] == cluster].index.tolist()\n",
    "    print(f\"\\nCluster {cluster}: {stocks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Volatility vs Return\n",
    "colors = ['red', 'blue', 'green']\n",
    "for cluster in range(3):\n",
    "    mask = cluster_df['Cluster'] == cluster\n",
    "    axes[0].scatter(\n",
    "        cluster_df.loc[mask, 'volatility'] * 100,\n",
    "        cluster_df.loc[mask, 'mean_return'] * 100,\n",
    "        c=colors[cluster], label=f'Cluster {cluster}', s=100, alpha=0.7\n",
    "    )\n",
    "    for ticker in cluster_df[mask].index:\n",
    "        axes[0].annotate(ticker, \n",
    "            (cluster_df.loc[ticker, 'volatility'] * 100, \n",
    "             cluster_df.loc[ticker, 'mean_return'] * 100),\n",
    "            fontsize=8, ha='center', va='bottom')\n",
    "\n",
    "axes[0].set_xlabel('Daily Volatility (%)')\n",
    "axes[0].set_ylabel('Mean Daily Return (%)')\n",
    "axes[0].set_title('Stock Clusters: Risk vs Return')\n",
    "axes[0].legend()\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 2: Cluster characteristics\n",
    "cluster_means = cluster_df.groupby('Cluster')[['mean_return', 'volatility', 'sharpe']].mean()\n",
    "cluster_means.plot(kind='bar', ax=axes[1], width=0.8)\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('Average Characteristics by Cluster')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA reduces dimensionality while preserving the most important patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to our feature set\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_cluster)\n",
    "\n",
    "print(\"Explained variance ratio:\")\n",
    "print(f\"PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "print(f\"Total: {sum(pca.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA results with clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for cluster in range(3):\n",
    "    mask = cluster_df['Cluster'] == cluster\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                c=colors[cluster], label=f'Cluster {cluster}', s=150, alpha=0.7)\n",
    "\n",
    "# Add stock labels\n",
    "for i, ticker in enumerate(cluster_df.index):\n",
    "    plt.annotate(ticker, (X_pca[i, 0], X_pca[i, 1]), \n",
    "                 fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('PCA: Stocks in 2D Space')\n",
    "plt.legend()\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding what the principal components represent\n",
    "pca_loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=cluster_df.columns[:-1]  # Exclude 'Cluster' column\n",
    ")\n",
    "\n",
    "print(\"PCA Loadings (what each feature contributes to each PC):\")\n",
    "print(pca_loadings.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Model Evaluation Best Practices\n",
    "\n",
    "Proper evaluation is crucial to avoid overfitting and ensure real-world performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Cross-Validation\n",
    "\n",
    "For time series, we use rolling/expanding windows instead of random folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit maintains temporal order\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Visualize the splits\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    ax.scatter(train_idx, [i] * len(train_idx), c='blue', s=1, label='Train' if i==0 else '')\n",
    "    ax.scatter(test_idx, [i] * len(test_idx), c='red', s=1, label='Test' if i==0 else '')\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('CV Fold')\n",
    "ax.set_title('Time Series Cross-Validation Splits')\n",
    "ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation scores for different models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=4, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "print(\"Cross-Validation Results (TimeSeriesSplit, 5 folds):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=tscv, scoring='accuracy')\n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    print(f\"  Fold scores: {[f'{s:.3f}' for s in scores]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "cv_df.index = [f'Fold {i+1}' for i in range(5)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "cv_df.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Cross-Validation Accuracy by Fold')\n",
    "ax.set_ylim(0.4, 0.7)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', label='Random guess')\n",
    "ax.legend(loc='lower right')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Look-Ahead Bias\n",
    "\n",
    "**Look-ahead bias** occurs when you use future information to make predictions about the past. Common mistakes:\n",
    "\n",
    "1. **Random train/test split** on time series data\n",
    "2. **Scaling** with test data included in the calculation\n",
    "3. **Feature engineering** using future values\n",
    "4. **Model selection** based on test set performance\n",
    "\n",
    "Always ask: \"Would I have this information at the time of prediction?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Practical Application - Building a Simple Trading Strategy\n",
    "\n",
    "Let's combine everything we've learned into a simple ML-based trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for backtesting\n",
    "backtest_df = df_clean.copy()\n",
    "backtest_df = backtest_df.iloc[split_index:]  # Use only test period\n",
    "\n",
    "# Get model predictions for test period\n",
    "backtest_df['Prediction'] = rf_model.predict(X_test_scaled)\n",
    "backtest_df['Probability'] = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f\"Backtesting period: {backtest_df.index[0].date()} to {backtest_df.index[-1].date()}\")\n",
    "print(f\"Number of trading days: {len(backtest_df)}\")\n",
    "backtest_df[['Close', 'Returns', 'Target', 'Prediction', 'Probability']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple strategy: \n",
    "# - Go LONG (hold stock) when model predicts UP\n",
    "# - Go to CASH when model predicts DOWN\n",
    "\n",
    "# Strategy returns: invest when signal is 1, stay in cash otherwise\n",
    "backtest_df['Strategy_Returns'] = backtest_df['Prediction'] * backtest_df['Returns']\n",
    "\n",
    "# Cumulative returns\n",
    "backtest_df['Cumulative_Market'] = (1 + backtest_df['Returns']).cumprod()\n",
    "backtest_df['Cumulative_Strategy'] = (1 + backtest_df['Strategy_Returns']).cumprod()\n",
    "\n",
    "# Calculate performance metrics\n",
    "total_market_return = backtest_df['Cumulative_Market'].iloc[-1] - 1\n",
    "total_strategy_return = backtest_df['Cumulative_Strategy'].iloc[-1] - 1\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"\\nBuy & Hold (Market):\")\n",
    "print(f\"  Total Return: {total_market_return:.2%}\")\n",
    "print(f\"  Annualized Return: {(backtest_df['Cumulative_Market'].iloc[-1] ** (252/len(backtest_df)) - 1):.2%}\")\n",
    "\n",
    "print(f\"\\nML Strategy:\")\n",
    "print(f\"  Total Return: {total_strategy_return:.2%}\")\n",
    "print(f\"  Annualized Return: {(backtest_df['Cumulative_Strategy'].iloc[-1] ** (252/len(backtest_df)) - 1):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize strategy performance\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Cumulative returns\n",
    "axes[0].plot(backtest_df.index, backtest_df['Cumulative_Market'], \n",
    "             label='Buy & Hold', linewidth=2)\n",
    "axes[0].plot(backtest_df.index, backtest_df['Cumulative_Strategy'], \n",
    "             label='ML Strategy', linewidth=2)\n",
    "axes[0].set_title('Cumulative Returns: ML Strategy vs Buy & Hold', fontsize=12)\n",
    "axes[0].set_ylabel('Portfolio Value (Starting $1)')\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 2: Trading signals\n",
    "axes[1].plot(backtest_df.index, backtest_df['Close'], label='Price', alpha=0.7)\n",
    "# Mark long positions\n",
    "long_mask = backtest_df['Prediction'] == 1\n",
    "axes[1].fill_between(backtest_df.index, backtest_df['Close'].min(), backtest_df['Close'].max(),\n",
    "                      where=long_mask, alpha=0.2, color='green', label='Long Position')\n",
    "axes[1].set_title('Trading Signals', fontsize=12)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Price ($)')\n",
    "axes[1].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional risk metrics\n",
    "strategy_returns = backtest_df['Strategy_Returns']\n",
    "market_returns = backtest_df['Returns']\n",
    "\n",
    "# Sharpe Ratio (assuming 0% risk-free rate for simplicity)\n",
    "sharpe_market = np.sqrt(252) * market_returns.mean() / market_returns.std()\n",
    "sharpe_strategy = np.sqrt(252) * strategy_returns.mean() / strategy_returns.std()\n",
    "\n",
    "# Maximum Drawdown\n",
    "def max_drawdown(cumulative_returns):\n",
    "    peak = cumulative_returns.cummax()\n",
    "    drawdown = (cumulative_returns - peak) / peak\n",
    "    return drawdown.min()\n",
    "\n",
    "mdd_market = max_drawdown(backtest_df['Cumulative_Market'])\n",
    "mdd_strategy = max_drawdown(backtest_df['Cumulative_Strategy'])\n",
    "\n",
    "# Win rate\n",
    "correct_predictions = (backtest_df['Prediction'] == backtest_df['Target']).sum()\n",
    "win_rate = correct_predictions / len(backtest_df)\n",
    "\n",
    "print(\"Risk Metrics Comparison:\")\n",
    "print(\"\\n{:<25} {:>15} {:>15}\".format('Metric', 'Buy & Hold', 'ML Strategy'))\n",
    "print(\"-\" * 55)\n",
    "print(\"{:<25} {:>15.2%} {:>15.2%}\".format('Total Return', total_market_return, total_strategy_return))\n",
    "print(\"{:<25} {:>15.2f} {:>15.2f}\".format('Sharpe Ratio', sharpe_market, sharpe_strategy))\n",
    "print(\"{:<25} {:>15.2%} {:>15.2%}\".format('Max Drawdown', mdd_market, mdd_strategy))\n",
    "print(\"{:<25} {:>15} {:>15.2%}\".format('Prediction Accuracy', 'N/A', win_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Caveats\n",
    "\n",
    "This is a simplified example. In real trading:\n",
    "\n",
    "1. **Transaction costs** - buying/selling has fees that eat into returns\n",
    "2. **Slippage** - you may not get the exact price you expect\n",
    "3. **Market impact** - large trades move prices\n",
    "4. **Survivorship bias** - we're testing on stocks that still exist\n",
    "5. **Overfitting risk** - past performance doesn't guarantee future results\n",
    "6. **Regime changes** - markets evolve, patterns may stop working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Next Steps and Resources\n",
    "\n",
    "Congratulations! You've learned the fundamentals of machine learning with financial applications. Here's where to go next:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Preview\n",
    "\n",
    "Neural networks can capture complex patterns:\n",
    "\n",
    "- **LSTM (Long Short-Term Memory)**: Great for sequential/time series data\n",
    "- **Transformer models**: State-of-the-art for many prediction tasks\n",
    "- **Autoencoders**: Anomaly detection in trading\n",
    "\n",
    "### Recommended Libraries\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|--------|\n",
    "| `ta-lib` | Technical analysis indicators |\n",
    "| `backtrader` | Backtesting trading strategies |\n",
    "| `zipline` | Algorithmic trading simulation |\n",
    "| `tensorflow` / `pytorch` | Deep learning |\n",
    "| `xgboost` / `lightgbm` | Gradient boosting (often wins competitions) |\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "1. **Overfitting**: Always validate on out-of-sample data\n",
    "2. **Look-ahead bias**: Never use future data in predictions\n",
    "3. **Survivorship bias**: Include delisted stocks in analysis\n",
    "4. **Ignoring transaction costs**: They can eliminate profits\n",
    "5. **Data snooping**: Don't test too many strategies on the same data\n",
    "6. **Overconfidence**: Markets are adversarial - other traders adapt\n",
    "\n",
    "### Further Learning\n",
    "\n",
    "- **Books**: \"Advances in Financial Machine Learning\" by Marcos López de Prado\n",
    "- **Courses**: Machine Learning for Trading (Coursera/Udacity)\n",
    "- **Practice**: Kaggle competitions on financial prediction\n",
    "- **Paper trading**: Test strategies with fake money before risking real capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we covered\n",
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║           MACHINE LEARNING FOR FINANCE - SUMMARY                 ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                  ║\n",
    "║  Part 1:  Environment Setup & Python Essentials                  ║\n",
    "║  Part 2:  Understanding Machine Learning                         ║\n",
    "║  Part 3:  Working with Financial Data                            ║\n",
    "║  Part 4:  Data Preprocessing                                     ║\n",
    "║  Part 5:  Supervised Learning - Regression                       ║\n",
    "║  Part 6:  Supervised Learning - Classification                   ║\n",
    "║  Part 7:  Unsupervised Learning                                  ║\n",
    "║  Part 8:  Model Evaluation Best Practices                        ║\n",
    "║  Part 9:  Building a Simple Trading Strategy                     ║\n",
    "║  Part 10: Next Steps and Resources                               ║\n",
    "║                                                                  ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                  ║\n",
    "║  Key Takeaways:                                                  ║\n",
    "║  • Always use temporal train/test splits for time series         ║\n",
    "║  • Feature engineering is crucial for financial ML               ║\n",
    "║  • Validate rigorously to avoid overfitting                      ║\n",
    "║  • Markets are hard to predict - even small edge is valuable     ║\n",
    "║  • Consider transaction costs and real-world constraints         ║\n",
    "║                                                                  ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n",
    "\n",
    "print(\"Happy learning and trading!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "1. **Feature Engineering**: Add more technical indicators (Bollinger Bands, MACD) and see if they improve model performance\n",
    "\n",
    "2. **Different Assets**: Apply the same analysis to cryptocurrency (BTC-USD) or forex data\n",
    "\n",
    "3. **Model Tuning**: Use GridSearchCV to find optimal hyperparameters for Random Forest\n",
    "\n",
    "4. **Multi-class Classification**: Instead of UP/DOWN, try predicting Strong Up, Weak Up, Weak Down, Strong Down\n",
    "\n",
    "5. **Portfolio Clustering**: Use clustering to build a diversified portfolio of uncorrelated stocks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
